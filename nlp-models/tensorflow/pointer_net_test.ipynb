{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Word Index Built\n",
      "==> Sequence Padded\n",
      "Epoch 1/300 | Batch 0/77 | train_loss: 2.702 | test_loss: 2.699\n",
      "Epoch 1/300 | Batch 50/77 | train_loss: 2.312 | test_loss: 2.289\n",
      "Epoch 2/300 | Batch 0/77 | train_loss: 2.047 | test_loss: 2.019\n",
      "Epoch 2/300 | Batch 50/77 | train_loss: 1.798 | test_loss: 1.709\n",
      "Epoch 3/300 | Batch 0/77 | train_loss: 1.645 | test_loss: 1.589\n",
      "Epoch 3/300 | Batch 50/77 | train_loss: 1.398 | test_loss: 1.292\n",
      "Epoch 4/300 | Batch 0/77 | train_loss: 1.124 | test_loss: 1.088\n",
      "Epoch 4/300 | Batch 50/77 | train_loss: 0.877 | test_loss: 0.820\n",
      "Epoch 5/300 | Batch 0/77 | train_loss: 0.761 | test_loss: 0.753\n",
      "Epoch 5/300 | Batch 50/77 | train_loss: 0.687 | test_loss: 0.651\n",
      "Epoch 6/300 | Batch 0/77 | train_loss: 0.629 | test_loss: 0.617\n",
      "Epoch 6/300 | Batch 50/77 | train_loss: 0.592 | test_loss: 0.566\n",
      "Epoch 7/300 | Batch 0/77 | train_loss: 0.548 | test_loss: 0.539\n",
      "Epoch 7/300 | Batch 50/77 | train_loss: 0.519 | test_loss: 0.504\n",
      "Epoch 8/300 | Batch 0/77 | train_loss: 0.489 | test_loss: 0.473\n",
      "Epoch 8/300 | Batch 50/77 | train_loss: 0.469 | test_loss: 0.443\n",
      "Epoch 9/300 | Batch 0/77 | train_loss: 0.436 | test_loss: 0.428\n",
      "Epoch 9/300 | Batch 50/77 | train_loss: 0.425 | test_loss: 0.405\n",
      "Epoch 10/300 | Batch 0/77 | train_loss: 0.404 | test_loss: 0.396\n",
      "Epoch 10/300 | Batch 50/77 | train_loss: 0.394 | test_loss: 0.376\n",
      "Epoch 11/300 | Batch 0/77 | train_loss: 0.379 | test_loss: 0.375\n",
      "Epoch 11/300 | Batch 50/77 | train_loss: 0.371 | test_loss: 0.354\n",
      "Epoch 12/300 | Batch 0/77 | train_loss: 0.371 | test_loss: 0.359\n",
      "Epoch 12/300 | Batch 50/77 | train_loss: 0.354 | test_loss: 0.338\n",
      "Epoch 13/300 | Batch 0/77 | train_loss: 0.341 | test_loss: 0.350\n",
      "Epoch 13/300 | Batch 50/77 | train_loss: 0.336 | test_loss: 0.328\n",
      "Epoch 14/300 | Batch 0/77 | train_loss: 0.329 | test_loss: 0.332\n",
      "Epoch 14/300 | Batch 50/77 | train_loss: 0.321 | test_loss: 0.312\n",
      "Epoch 15/300 | Batch 0/77 | train_loss: 0.318 | test_loss: 0.322\n",
      "Epoch 15/300 | Batch 50/77 | train_loss: 0.308 | test_loss: 0.302\n",
      "Epoch 16/300 | Batch 0/77 | train_loss: 0.309 | test_loss: 0.311\n",
      "Epoch 16/300 | Batch 50/77 | train_loss: 0.297 | test_loss: 0.294\n",
      "Epoch 17/300 | Batch 0/77 | train_loss: 0.303 | test_loss: 0.305\n",
      "Epoch 17/300 | Batch 50/77 | train_loss: 0.288 | test_loss: 0.286\n",
      "Epoch 18/300 | Batch 0/77 | train_loss: 0.300 | test_loss: 0.289\n",
      "Epoch 18/300 | Batch 50/77 | train_loss: 0.282 | test_loss: 0.280\n",
      "Epoch 19/300 | Batch 0/77 | train_loss: 0.294 | test_loss: 0.281\n",
      "Epoch 19/300 | Batch 50/77 | train_loss: 0.276 | test_loss: 0.272\n",
      "Epoch 20/300 | Batch 0/77 | train_loss: 0.278 | test_loss: 0.274\n",
      "Epoch 20/300 | Batch 50/77 | train_loss: 0.268 | test_loss: 0.265\n",
      "Epoch 21/300 | Batch 0/77 | train_loss: 0.270 | test_loss: 0.269\n",
      "Epoch 21/300 | Batch 50/77 | train_loss: 0.261 | test_loss: 0.258\n",
      "Epoch 22/300 | Batch 0/77 | train_loss: 0.264 | test_loss: 0.263\n",
      "Epoch 22/300 | Batch 50/77 | train_loss: 0.253 | test_loss: 0.253\n",
      "Epoch 23/300 | Batch 0/77 | train_loss: 0.259 | test_loss: 0.258\n",
      "Epoch 23/300 | Batch 50/77 | train_loss: 0.244 | test_loss: 0.248\n",
      "Epoch 24/300 | Batch 0/77 | train_loss: 0.254 | test_loss: 0.252\n",
      "Epoch 24/300 | Batch 50/77 | train_loss: 0.237 | test_loss: 0.242\n",
      "Epoch 25/300 | Batch 0/77 | train_loss: 0.250 | test_loss: 0.247\n",
      "Epoch 25/300 | Batch 50/77 | train_loss: 0.231 | test_loss: 0.236\n",
      "Epoch 26/300 | Batch 0/77 | train_loss: 0.251 | test_loss: 0.241\n",
      "Epoch 26/300 | Batch 50/77 | train_loss: 0.226 | test_loss: 0.231\n",
      "Epoch 27/300 | Batch 0/77 | train_loss: 0.251 | test_loss: 0.236\n",
      "Epoch 27/300 | Batch 50/77 | train_loss: 0.223 | test_loss: 0.227\n",
      "Epoch 28/300 | Batch 0/77 | train_loss: 0.256 | test_loss: 0.229\n",
      "Epoch 28/300 | Batch 50/77 | train_loss: 0.214 | test_loss: 0.225\n",
      "Epoch 29/300 | Batch 0/77 | train_loss: 0.284 | test_loss: 0.230\n",
      "Epoch 29/300 | Batch 50/77 | train_loss: 0.211 | test_loss: 0.222\n",
      "Epoch 30/300 | Batch 0/77 | train_loss: 0.222 | test_loss: 0.220\n",
      "Epoch 30/300 | Batch 50/77 | train_loss: 0.209 | test_loss: 0.220\n",
      "Epoch 31/300 | Batch 0/77 | train_loss: 0.220 | test_loss: 0.219\n",
      "Epoch 31/300 | Batch 50/77 | train_loss: 0.204 | test_loss: 0.218\n",
      "Epoch 32/300 | Batch 0/77 | train_loss: 0.217 | test_loss: 0.216\n",
      "Epoch 32/300 | Batch 50/77 | train_loss: 0.201 | test_loss: 0.215\n",
      "Epoch 33/300 | Batch 0/77 | train_loss: 0.214 | test_loss: 0.212\n",
      "Epoch 33/300 | Batch 50/77 | train_loss: 0.197 | test_loss: 0.212\n",
      "Epoch 34/300 | Batch 0/77 | train_loss: 0.209 | test_loss: 0.210\n",
      "Epoch 34/300 | Batch 50/77 | train_loss: 0.194 | test_loss: 0.209\n",
      "Epoch 35/300 | Batch 0/77 | train_loss: 0.206 | test_loss: 0.207\n",
      "Epoch 35/300 | Batch 50/77 | train_loss: 0.191 | test_loss: 0.206\n",
      "Epoch 36/300 | Batch 0/77 | train_loss: 0.202 | test_loss: 0.203\n",
      "Epoch 36/300 | Batch 50/77 | train_loss: 0.188 | test_loss: 0.203\n",
      "Epoch 37/300 | Batch 0/77 | train_loss: 0.199 | test_loss: 0.200\n",
      "Epoch 37/300 | Batch 50/77 | train_loss: 0.185 | test_loss: 0.201\n",
      "Epoch 38/300 | Batch 0/77 | train_loss: 0.196 | test_loss: 0.197\n",
      "Epoch 38/300 | Batch 50/77 | train_loss: 0.182 | test_loss: 0.200\n",
      "Epoch 39/300 | Batch 0/77 | train_loss: 0.193 | test_loss: 0.195\n",
      "Epoch 39/300 | Batch 50/77 | train_loss: 0.179 | test_loss: 0.198\n",
      "Epoch 40/300 | Batch 0/77 | train_loss: 0.191 | test_loss: 0.193\n",
      "Epoch 40/300 | Batch 50/77 | train_loss: 0.176 | test_loss: 0.195\n",
      "Epoch 41/300 | Batch 0/77 | train_loss: 0.189 | test_loss: 0.191\n",
      "Epoch 41/300 | Batch 50/77 | train_loss: 0.174 | test_loss: 0.192\n",
      "Epoch 42/300 | Batch 0/77 | train_loss: 0.187 | test_loss: 0.190\n",
      "Epoch 42/300 | Batch 50/77 | train_loss: 0.171 | test_loss: 0.191\n",
      "Epoch 43/300 | Batch 0/77 | train_loss: 0.185 | test_loss: 0.187\n",
      "Epoch 43/300 | Batch 50/77 | train_loss: 0.169 | test_loss: 0.193\n",
      "Epoch 44/300 | Batch 0/77 | train_loss: 0.185 | test_loss: 0.185\n",
      "Epoch 44/300 | Batch 50/77 | train_loss: 0.166 | test_loss: 0.193\n",
      "Epoch 45/300 | Batch 0/77 | train_loss: 0.184 | test_loss: 0.184\n",
      "Epoch 45/300 | Batch 50/77 | train_loss: 0.163 | test_loss: 0.193\n",
      "Epoch 46/300 | Batch 0/77 | train_loss: 0.185 | test_loss: 0.184\n",
      "Epoch 46/300 | Batch 50/77 | train_loss: 0.161 | test_loss: 0.184\n",
      "Epoch 47/300 | Batch 0/77 | train_loss: 0.187 | test_loss: 0.181\n",
      "Epoch 47/300 | Batch 50/77 | train_loss: 0.159 | test_loss: 0.176\n",
      "Epoch 48/300 | Batch 0/77 | train_loss: 0.240 | test_loss: 0.237\n",
      "Epoch 48/300 | Batch 50/77 | train_loss: 0.158 | test_loss: 0.174\n",
      "Epoch 49/300 | Batch 0/77 | train_loss: 0.185 | test_loss: 0.181\n",
      "Epoch 49/300 | Batch 50/77 | train_loss: 0.157 | test_loss: 0.178\n",
      "Epoch 50/300 | Batch 0/77 | train_loss: 0.175 | test_loss: 0.177\n",
      "Epoch 50/300 | Batch 50/77 | train_loss: 0.155 | test_loss: 0.164\n",
      "Epoch 51/300 | Batch 0/77 | train_loss: 0.165 | test_loss: 0.168\n",
      "Epoch 51/300 | Batch 50/77 | train_loss: 0.153 | test_loss: 0.165\n",
      "Epoch 52/300 | Batch 0/77 | train_loss: 0.169 | test_loss: 0.171\n",
      "Epoch 52/300 | Batch 50/77 | train_loss: 0.151 | test_loss: 0.167\n",
      "Epoch 53/300 | Batch 0/77 | train_loss: 0.168 | test_loss: 0.169\n",
      "Epoch 53/300 | Batch 50/77 | train_loss: 0.150 | test_loss: 0.161\n",
      "Epoch 54/300 | Batch 0/77 | train_loss: 0.163 | test_loss: 0.166\n",
      "Epoch 54/300 | Batch 50/77 | train_loss: 0.148 | test_loss: 0.163\n",
      "Epoch 55/300 | Batch 0/77 | train_loss: 0.160 | test_loss: 0.164\n",
      "Epoch 55/300 | Batch 50/77 | train_loss: 0.147 | test_loss: 0.162\n",
      "Epoch 56/300 | Batch 0/77 | train_loss: 0.159 | test_loss: 0.162\n",
      "Epoch 56/300 | Batch 50/77 | train_loss: 0.145 | test_loss: 0.161\n",
      "Epoch 57/300 | Batch 0/77 | train_loss: 0.157 | test_loss: 0.161\n",
      "Epoch 57/300 | Batch 50/77 | train_loss: 0.144 | test_loss: 0.160\n",
      "Epoch 58/300 | Batch 0/77 | train_loss: 0.156 | test_loss: 0.159\n",
      "Epoch 58/300 | Batch 50/77 | train_loss: 0.143 | test_loss: 0.159\n",
      "Epoch 59/300 | Batch 0/77 | train_loss: 0.155 | test_loss: 0.157\n",
      "Epoch 59/300 | Batch 50/77 | train_loss: 0.142 | test_loss: 0.158\n",
      "Epoch 60/300 | Batch 0/77 | train_loss: 0.153 | test_loss: 0.155\n",
      "Epoch 60/300 | Batch 50/77 | train_loss: 0.141 | test_loss: 0.157\n",
      "Epoch 61/300 | Batch 0/77 | train_loss: 0.151 | test_loss: 0.155\n",
      "Epoch 61/300 | Batch 50/77 | train_loss: 0.140 | test_loss: 0.156\n",
      "Epoch 62/300 | Batch 0/77 | train_loss: 0.149 | test_loss: 0.155\n",
      "Epoch 62/300 | Batch 50/77 | train_loss: 0.139 | test_loss: 0.156\n",
      "Epoch 63/300 | Batch 0/77 | train_loss: 0.148 | test_loss: 0.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 | Batch 50/77 | train_loss: 0.138 | test_loss: 0.156\n",
      "Epoch 64/300 | Batch 0/77 | train_loss: 0.147 | test_loss: 0.152\n",
      "Epoch 64/300 | Batch 50/77 | train_loss: 0.137 | test_loss: 0.154\n",
      "Epoch 65/300 | Batch 0/77 | train_loss: 0.146 | test_loss: 0.153\n",
      "Epoch 65/300 | Batch 50/77 | train_loss: 0.137 | test_loss: 0.157\n",
      "Epoch 66/300 | Batch 0/77 | train_loss: 0.145 | test_loss: 0.152\n",
      "Epoch 66/300 | Batch 50/77 | train_loss: 0.135 | test_loss: 0.151\n",
      "Epoch 67/300 | Batch 0/77 | train_loss: 0.143 | test_loss: 0.148\n",
      "Epoch 67/300 | Batch 50/77 | train_loss: 0.135 | test_loss: 0.154\n",
      "Epoch 68/300 | Batch 0/77 | train_loss: 0.142 | test_loss: 0.150\n",
      "Epoch 68/300 | Batch 50/77 | train_loss: 0.133 | test_loss: 0.152\n",
      "Epoch 69/300 | Batch 0/77 | train_loss: 0.141 | test_loss: 0.148\n",
      "Epoch 69/300 | Batch 50/77 | train_loss: 0.132 | test_loss: 0.151\n",
      "Epoch 70/300 | Batch 0/77 | train_loss: 0.140 | test_loss: 0.147\n",
      "Epoch 70/300 | Batch 50/77 | train_loss: 0.131 | test_loss: 0.148\n",
      "Epoch 71/300 | Batch 0/77 | train_loss: 0.139 | test_loss: 0.144\n",
      "Epoch 71/300 | Batch 50/77 | train_loss: 0.131 | test_loss: 0.146\n",
      "Epoch 72/300 | Batch 0/77 | train_loss: 0.138 | test_loss: 0.145\n",
      "Epoch 72/300 | Batch 50/77 | train_loss: 0.134 | test_loss: 0.146\n",
      "Epoch 73/300 | Batch 0/77 | train_loss: 0.137 | test_loss: 0.142\n",
      "Epoch 73/300 | Batch 50/77 | train_loss: 0.133 | test_loss: 0.153\n",
      "Epoch 74/300 | Batch 0/77 | train_loss: 0.137 | test_loss: 0.141\n",
      "Epoch 74/300 | Batch 50/77 | train_loss: 0.131 | test_loss: 0.148\n",
      "Epoch 75/300 | Batch 0/77 | train_loss: 0.137 | test_loss: 0.142\n",
      "Epoch 75/300 | Batch 50/77 | train_loss: 0.133 | test_loss: 0.154\n",
      "Epoch 76/300 | Batch 0/77 | train_loss: 0.138 | test_loss: 0.147\n",
      "Epoch 76/300 | Batch 50/77 | train_loss: 0.128 | test_loss: 0.157\n",
      "Epoch 77/300 | Batch 0/77 | train_loss: 0.148 | test_loss: 0.142\n",
      "Epoch 77/300 | Batch 50/77 | train_loss: 0.131 | test_loss: 0.147\n",
      "Epoch 78/300 | Batch 0/77 | train_loss: 0.165 | test_loss: 0.189\n",
      "Epoch 78/300 | Batch 50/77 | train_loss: 0.141 | test_loss: 0.144\n",
      "Epoch 79/300 | Batch 0/77 | train_loss: 0.131 | test_loss: 0.139\n",
      "Epoch 79/300 | Batch 50/77 | train_loss: 0.127 | test_loss: 0.143\n",
      "Epoch 80/300 | Batch 0/77 | train_loss: 0.130 | test_loss: 0.139\n",
      "Epoch 80/300 | Batch 50/77 | train_loss: 0.133 | test_loss: 0.136\n",
      "Epoch 81/300 | Batch 0/77 | train_loss: 0.131 | test_loss: 0.138\n",
      "Epoch 81/300 | Batch 50/77 | train_loss: 0.136 | test_loss: 0.135\n",
      "Epoch 82/300 | Batch 0/77 | train_loss: 0.133 | test_loss: 0.142\n",
      "Epoch 82/300 | Batch 50/77 | train_loss: 0.135 | test_loss: 0.140\n",
      "Epoch 83/300 | Batch 0/77 | train_loss: 0.135 | test_loss: 0.154\n",
      "Epoch 83/300 | Batch 50/77 | train_loss: 0.124 | test_loss: 0.134\n",
      "Epoch 84/300 | Batch 0/77 | train_loss: 0.129 | test_loss: 0.148\n",
      "Epoch 84/300 | Batch 50/77 | train_loss: 0.121 | test_loss: 0.132\n",
      "Epoch 85/300 | Batch 0/77 | train_loss: 0.126 | test_loss: 0.134\n",
      "Epoch 85/300 | Batch 50/77 | train_loss: 0.125 | test_loss: 0.130\n",
      "Epoch 86/300 | Batch 0/77 | train_loss: 0.125 | test_loss: 0.133\n",
      "Epoch 86/300 | Batch 50/77 | train_loss: 0.124 | test_loss: 0.131\n",
      "Epoch 87/300 | Batch 0/77 | train_loss: 0.125 | test_loss: 0.135\n",
      "Epoch 87/300 | Batch 50/77 | train_loss: 0.123 | test_loss: 0.131\n",
      "Epoch 88/300 | Batch 0/77 | train_loss: 0.124 | test_loss: 0.133\n",
      "Epoch 88/300 | Batch 50/77 | train_loss: 0.123 | test_loss: 0.131\n",
      "Epoch 89/300 | Batch 0/77 | train_loss: 0.123 | test_loss: 0.133\n",
      "Epoch 89/300 | Batch 50/77 | train_loss: 0.123 | test_loss: 0.131\n",
      "Epoch 90/300 | Batch 0/77 | train_loss: 0.122 | test_loss: 0.133\n",
      "Epoch 90/300 | Batch 50/77 | train_loss: 0.121 | test_loss: 0.131\n",
      "Epoch 91/300 | Batch 0/77 | train_loss: 0.122 | test_loss: 0.133\n",
      "Epoch 91/300 | Batch 50/77 | train_loss: 0.119 | test_loss: 0.131\n",
      "Epoch 92/300 | Batch 0/77 | train_loss: 0.122 | test_loss: 0.132\n",
      "Epoch 92/300 | Batch 50/77 | train_loss: 0.117 | test_loss: 0.130\n",
      "Epoch 93/300 | Batch 0/77 | train_loss: 0.125 | test_loss: 0.144\n",
      "Epoch 93/300 | Batch 50/77 | train_loss: 0.116 | test_loss: 0.129\n",
      "Epoch 94/300 | Batch 0/77 | train_loss: 0.123 | test_loss: 0.132\n",
      "Epoch 94/300 | Batch 50/77 | train_loss: 0.116 | test_loss: 0.127\n",
      "Epoch 95/300 | Batch 0/77 | train_loss: 0.125 | test_loss: 0.144\n",
      "Epoch 95/300 | Batch 50/77 | train_loss: 0.114 | test_loss: 0.128\n",
      "Epoch 96/300 | Batch 0/77 | train_loss: 0.121 | test_loss: 0.137\n",
      "Epoch 96/300 | Batch 50/77 | train_loss: 0.115 | test_loss: 0.128\n",
      "Epoch 97/300 | Batch 0/77 | train_loss: 0.124 | test_loss: 0.140\n",
      "Epoch 97/300 | Batch 50/77 | train_loss: 0.114 | test_loss: 0.129\n",
      "Epoch 98/300 | Batch 0/77 | train_loss: 0.118 | test_loss: 0.132\n",
      "Epoch 98/300 | Batch 50/77 | train_loss: 0.116 | test_loss: 0.128\n",
      "Epoch 99/300 | Batch 0/77 | train_loss: 0.116 | test_loss: 0.134\n",
      "Epoch 99/300 | Batch 50/77 | train_loss: 0.113 | test_loss: 0.134\n",
      "Epoch 100/300 | Batch 0/77 | train_loss: 0.116 | test_loss: 0.128\n",
      "Epoch 100/300 | Batch 50/77 | train_loss: 0.113 | test_loss: 0.129\n",
      "Epoch 101/300 | Batch 0/77 | train_loss: 0.115 | test_loss: 0.125\n",
      "Epoch 101/300 | Batch 50/77 | train_loss: 0.112 | test_loss: 0.141\n",
      "Epoch 102/300 | Batch 0/77 | train_loss: 0.118 | test_loss: 0.127\n",
      "Epoch 102/300 | Batch 50/77 | train_loss: 0.111 | test_loss: 0.138\n",
      "Epoch 103/300 | Batch 0/77 | train_loss: 0.118 | test_loss: 0.130\n",
      "Epoch 103/300 | Batch 50/77 | train_loss: 0.112 | test_loss: 0.131\n",
      "Epoch 104/300 | Batch 0/77 | train_loss: 0.114 | test_loss: 0.131\n",
      "Epoch 104/300 | Batch 50/77 | train_loss: 0.111 | test_loss: 0.133\n",
      "Epoch 105/300 | Batch 0/77 | train_loss: 0.113 | test_loss: 0.126\n",
      "Epoch 105/300 | Batch 50/77 | train_loss: 0.108 | test_loss: 0.134\n",
      "Epoch 106/300 | Batch 0/77 | train_loss: 0.125 | test_loss: 0.133\n",
      "Epoch 106/300 | Batch 50/77 | train_loss: 0.109 | test_loss: 0.142\n",
      "Epoch 107/300 | Batch 0/77 | train_loss: 0.120 | test_loss: 0.138\n",
      "Epoch 107/300 | Batch 50/77 | train_loss: 0.111 | test_loss: 0.125\n",
      "Epoch 108/300 | Batch 0/77 | train_loss: 0.116 | test_loss: 0.154\n",
      "Epoch 108/300 | Batch 50/77 | train_loss: 0.108 | test_loss: 0.121\n",
      "Epoch 109/300 | Batch 0/77 | train_loss: 0.113 | test_loss: 0.147\n",
      "Epoch 109/300 | Batch 50/77 | train_loss: 0.106 | test_loss: 0.120\n",
      "Epoch 110/300 | Batch 0/77 | train_loss: 0.123 | test_loss: 0.164\n",
      "Epoch 110/300 | Batch 50/77 | train_loss: 0.108 | test_loss: 0.124\n",
      "Epoch 111/300 | Batch 0/77 | train_loss: 0.115 | test_loss: 0.159\n",
      "Epoch 111/300 | Batch 50/77 | train_loss: 0.129 | test_loss: 0.127\n",
      "Epoch 112/300 | Batch 0/77 | train_loss: 0.112 | test_loss: 0.126\n",
      "Epoch 112/300 | Batch 50/77 | train_loss: 0.105 | test_loss: 0.118\n",
      "Epoch 113/300 | Batch 0/77 | train_loss: 0.110 | test_loss: 0.117\n",
      "Epoch 113/300 | Batch 50/77 | train_loss: 0.101 | test_loss: 0.117\n",
      "Epoch 114/300 | Batch 0/77 | train_loss: 0.108 | test_loss: 0.116\n",
      "Epoch 114/300 | Batch 50/77 | train_loss: 0.102 | test_loss: 0.119\n",
      "Epoch 115/300 | Batch 0/77 | train_loss: 0.107 | test_loss: 0.115\n",
      "Epoch 115/300 | Batch 50/77 | train_loss: 0.100 | test_loss: 0.116\n",
      "Epoch 116/300 | Batch 0/77 | train_loss: 0.111 | test_loss: 0.122\n",
      "Epoch 116/300 | Batch 50/77 | train_loss: 0.100 | test_loss: 0.116\n",
      "Epoch 117/300 | Batch 0/77 | train_loss: 0.104 | test_loss: 0.118\n",
      "Epoch 117/300 | Batch 50/77 | train_loss: 0.100 | test_loss: 0.116\n",
      "Epoch 118/300 | Batch 0/77 | train_loss: 0.104 | test_loss: 0.114\n",
      "Epoch 118/300 | Batch 50/77 | train_loss: 0.099 | test_loss: 0.117\n",
      "Epoch 119/300 | Batch 0/77 | train_loss: 0.104 | test_loss: 0.113\n",
      "Epoch 119/300 | Batch 50/77 | train_loss: 0.100 | test_loss: 0.115\n",
      "Epoch 120/300 | Batch 0/77 | train_loss: 0.105 | test_loss: 0.115\n",
      "Epoch 120/300 | Batch 50/77 | train_loss: 0.098 | test_loss: 0.114\n",
      "Epoch 121/300 | Batch 0/77 | train_loss: 0.104 | test_loss: 0.118\n",
      "Epoch 121/300 | Batch 50/77 | train_loss: 0.098 | test_loss: 0.114\n",
      "Epoch 122/300 | Batch 0/77 | train_loss: 0.102 | test_loss: 0.114\n",
      "Epoch 122/300 | Batch 50/77 | train_loss: 0.101 | test_loss: 0.124\n",
      "Epoch 123/300 | Batch 0/77 | train_loss: 0.100 | test_loss: 0.114\n",
      "Epoch 123/300 | Batch 50/77 | train_loss: 0.096 | test_loss: 0.112\n",
      "Epoch 124/300 | Batch 0/77 | train_loss: 0.118 | test_loss: 0.115\n",
      "Epoch 124/300 | Batch 50/77 | train_loss: 0.097 | test_loss: 0.115\n",
      "Epoch 125/300 | Batch 0/77 | train_loss: 0.099 | test_loss: 0.116\n",
      "Epoch 125/300 | Batch 50/77 | train_loss: 0.096 | test_loss: 0.116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/300 | Batch 0/77 | train_loss: 0.100 | test_loss: 0.112\n",
      "Epoch 126/300 | Batch 50/77 | train_loss: 0.097 | test_loss: 0.115\n",
      "Epoch 127/300 | Batch 0/77 | train_loss: 0.098 | test_loss: 0.116\n",
      "Epoch 127/300 | Batch 50/77 | train_loss: 0.095 | test_loss: 0.115\n",
      "Epoch 128/300 | Batch 0/77 | train_loss: 0.098 | test_loss: 0.110\n",
      "Epoch 128/300 | Batch 50/77 | train_loss: 0.094 | test_loss: 0.114\n",
      "Epoch 129/300 | Batch 0/77 | train_loss: 0.097 | test_loss: 0.113\n",
      "Epoch 129/300 | Batch 50/77 | train_loss: 0.102 | test_loss: 0.113\n",
      "Epoch 130/300 | Batch 0/77 | train_loss: 0.097 | test_loss: 0.113\n",
      "Epoch 130/300 | Batch 50/77 | train_loss: 0.095 | test_loss: 0.114\n",
      "Epoch 131/300 | Batch 0/77 | train_loss: 0.096 | test_loss: 0.110\n",
      "Epoch 131/300 | Batch 50/77 | train_loss: 0.094 | test_loss: 0.112\n",
      "Epoch 132/300 | Batch 0/77 | train_loss: 0.100 | test_loss: 0.113\n",
      "Epoch 132/300 | Batch 50/77 | train_loss: 0.093 | test_loss: 0.114\n",
      "Epoch 133/300 | Batch 0/77 | train_loss: 0.095 | test_loss: 0.116\n",
      "Epoch 133/300 | Batch 50/77 | train_loss: 0.092 | test_loss: 0.117\n",
      "Epoch 134/300 | Batch 0/77 | train_loss: 0.101 | test_loss: 0.109\n",
      "Epoch 134/300 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.112\n",
      "Epoch 135/300 | Batch 0/77 | train_loss: 0.097 | test_loss: 0.109\n",
      "Epoch 135/300 | Batch 50/77 | train_loss: 0.093 | test_loss: 0.114\n",
      "Epoch 136/300 | Batch 0/77 | train_loss: 0.095 | test_loss: 0.109\n",
      "Epoch 136/300 | Batch 50/77 | train_loss: 0.092 | test_loss: 0.114\n",
      "Epoch 137/300 | Batch 0/77 | train_loss: 0.092 | test_loss: 0.110\n",
      "Epoch 137/300 | Batch 50/77 | train_loss: 0.089 | test_loss: 0.116\n",
      "Epoch 138/300 | Batch 0/77 | train_loss: 0.095 | test_loss: 0.109\n",
      "Epoch 138/300 | Batch 50/77 | train_loss: 0.090 | test_loss: 0.112\n",
      "Epoch 139/300 | Batch 0/77 | train_loss: 0.094 | test_loss: 0.111\n",
      "Epoch 139/300 | Batch 50/77 | train_loss: 0.092 | test_loss: 0.115\n",
      "Epoch 140/300 | Batch 0/77 | train_loss: 0.092 | test_loss: 0.117\n",
      "Epoch 140/300 | Batch 50/77 | train_loss: 0.089 | test_loss: 0.116\n",
      "Epoch 141/300 | Batch 0/77 | train_loss: 0.091 | test_loss: 0.121\n",
      "Epoch 141/300 | Batch 50/77 | train_loss: 0.089 | test_loss: 0.111\n",
      "Epoch 142/300 | Batch 0/77 | train_loss: 0.094 | test_loss: 0.119\n",
      "Epoch 142/300 | Batch 50/77 | train_loss: 0.117 | test_loss: 0.112\n",
      "Epoch 143/300 | Batch 0/77 | train_loss: 0.110 | test_loss: 0.166\n",
      "Epoch 143/300 | Batch 50/77 | train_loss: 0.106 | test_loss: 0.130\n",
      "Epoch 144/300 | Batch 0/77 | train_loss: 0.096 | test_loss: 0.117\n",
      "Epoch 144/300 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.109\n",
      "Epoch 145/300 | Batch 0/77 | train_loss: 0.091 | test_loss: 0.112\n",
      "Epoch 145/300 | Batch 50/77 | train_loss: 0.101 | test_loss: 0.103\n",
      "Epoch 146/300 | Batch 0/77 | train_loss: 0.103 | test_loss: 0.123\n",
      "Epoch 146/300 | Batch 50/77 | train_loss: 0.094 | test_loss: 0.104\n",
      "Epoch 147/300 | Batch 0/77 | train_loss: 0.102 | test_loss: 0.156\n",
      "Epoch 147/300 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.104\n",
      "Epoch 148/300 | Batch 0/77 | train_loss: 0.090 | test_loss: 0.123\n",
      "Epoch 148/300 | Batch 50/77 | train_loss: 0.087 | test_loss: 0.109\n",
      "Epoch 149/300 | Batch 0/77 | train_loss: 0.092 | test_loss: 0.115\n",
      "Epoch 149/300 | Batch 50/77 | train_loss: 0.085 | test_loss: 0.104\n",
      "Epoch 150/300 | Batch 0/77 | train_loss: 0.092 | test_loss: 0.107\n",
      "Epoch 150/300 | Batch 50/77 | train_loss: 0.084 | test_loss: 0.110\n",
      "Epoch 151/300 | Batch 0/77 | train_loss: 0.088 | test_loss: 0.104\n",
      "Epoch 151/300 | Batch 50/77 | train_loss: 0.083 | test_loss: 0.114\n",
      "Epoch 152/300 | Batch 0/77 | train_loss: 0.087 | test_loss: 0.103\n",
      "Epoch 152/300 | Batch 50/77 | train_loss: 0.082 | test_loss: 0.114\n",
      "Epoch 153/300 | Batch 0/77 | train_loss: 0.087 | test_loss: 0.102\n",
      "Epoch 153/300 | Batch 50/77 | train_loss: 0.084 | test_loss: 0.117\n",
      "Epoch 154/300 | Batch 0/77 | train_loss: 0.085 | test_loss: 0.111\n",
      "Epoch 154/300 | Batch 50/77 | train_loss: 0.084 | test_loss: 0.118\n",
      "Epoch 155/300 | Batch 0/77 | train_loss: 0.085 | test_loss: 0.109\n",
      "Epoch 155/300 | Batch 50/77 | train_loss: 0.082 | test_loss: 0.103\n",
      "Epoch 156/300 | Batch 0/77 | train_loss: 0.085 | test_loss: 0.112\n",
      "Epoch 156/300 | Batch 50/77 | train_loss: 0.082 | test_loss: 0.103\n",
      "Epoch 157/300 | Batch 0/77 | train_loss: 0.084 | test_loss: 0.112\n",
      "Epoch 157/300 | Batch 50/77 | train_loss: 0.082 | test_loss: 0.102\n",
      "Epoch 158/300 | Batch 0/77 | train_loss: 0.084 | test_loss: 0.112\n",
      "Epoch 158/300 | Batch 50/77 | train_loss: 0.081 | test_loss: 0.103\n",
      "Epoch 159/300 | Batch 0/77 | train_loss: 0.083 | test_loss: 0.114\n",
      "Epoch 159/300 | Batch 50/77 | train_loss: 0.081 | test_loss: 0.119\n",
      "Epoch 160/300 | Batch 0/77 | train_loss: 0.083 | test_loss: 0.113\n",
      "Epoch 160/300 | Batch 50/77 | train_loss: 0.079 | test_loss: 0.102\n",
      "Epoch 161/300 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.113\n",
      "Epoch 161/300 | Batch 50/77 | train_loss: 0.081 | test_loss: 0.101\n",
      "Epoch 162/300 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.110\n",
      "Epoch 162/300 | Batch 50/77 | train_loss: 0.078 | test_loss: 0.105\n",
      "Epoch 163/300 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.127\n",
      "Epoch 163/300 | Batch 50/77 | train_loss: 0.079 | test_loss: 0.101\n",
      "Epoch 164/300 | Batch 0/77 | train_loss: 0.080 | test_loss: 0.107\n",
      "Epoch 164/300 | Batch 50/77 | train_loss: 0.078 | test_loss: 0.101\n",
      "Epoch 165/300 | Batch 0/77 | train_loss: 0.081 | test_loss: 0.116\n",
      "Epoch 165/300 | Batch 50/77 | train_loss: 0.079 | test_loss: 0.098\n",
      "Epoch 166/300 | Batch 0/77 | train_loss: 0.079 | test_loss: 0.114\n",
      "Epoch 166/300 | Batch 50/77 | train_loss: 0.079 | test_loss: 0.102\n",
      "Epoch 167/300 | Batch 0/77 | train_loss: 0.080 | test_loss: 0.109\n",
      "Epoch 167/300 | Batch 50/77 | train_loss: 0.079 | test_loss: 0.099\n",
      "Epoch 168/300 | Batch 0/77 | train_loss: 0.081 | test_loss: 0.115\n",
      "Epoch 168/300 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.117\n",
      "Epoch 169/300 | Batch 0/77 | train_loss: 0.079 | test_loss: 0.107\n",
      "Epoch 169/300 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.117\n",
      "Epoch 170/300 | Batch 0/77 | train_loss: 0.079 | test_loss: 0.102\n",
      "Epoch 170/300 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.105\n",
      "Epoch 171/300 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.115\n",
      "Epoch 171/300 | Batch 50/77 | train_loss: 0.081 | test_loss: 0.105\n",
      "Epoch 172/300 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.119\n",
      "Epoch 172/300 | Batch 50/77 | train_loss: 0.078 | test_loss: 0.143\n",
      "Epoch 173/300 | Batch 0/77 | train_loss: 0.090 | test_loss: 0.140\n",
      "Epoch 173/300 | Batch 50/77 | train_loss: 0.090 | test_loss: 0.098\n",
      "Epoch 174/300 | Batch 0/77 | train_loss: 0.110 | test_loss: 0.102\n",
      "Epoch 174/300 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.101\n",
      "Epoch 175/300 | Batch 0/77 | train_loss: 0.087 | test_loss: 0.108\n",
      "Epoch 175/300 | Batch 50/77 | train_loss: 0.085 | test_loss: 0.116\n",
      "Epoch 176/300 | Batch 0/77 | train_loss: 0.085 | test_loss: 0.119\n",
      "Epoch 176/300 | Batch 50/77 | train_loss: 0.079 | test_loss: 0.119\n",
      "Epoch 177/300 | Batch 0/77 | train_loss: 0.079 | test_loss: 0.100\n",
      "Epoch 177/300 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.115\n",
      "Epoch 178/300 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.102\n",
      "Epoch 178/300 | Batch 50/77 | train_loss: 0.079 | test_loss: 0.112\n",
      "Epoch 179/300 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.107\n",
      "Epoch 179/300 | Batch 50/77 | train_loss: 0.076 | test_loss: 0.116\n",
      "Epoch 180/300 | Batch 0/77 | train_loss: 0.075 | test_loss: 0.102\n",
      "Epoch 180/300 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.126\n",
      "Epoch 181/300 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.105\n",
      "Epoch 181/300 | Batch 50/77 | train_loss: 0.072 | test_loss: 0.114\n",
      "Epoch 182/300 | Batch 0/77 | train_loss: 0.075 | test_loss: 0.110\n",
      "Epoch 182/300 | Batch 50/77 | train_loss: 0.072 | test_loss: 0.112\n",
      "Epoch 183/300 | Batch 0/77 | train_loss: 0.075 | test_loss: 0.109\n",
      "Epoch 183/300 | Batch 50/77 | train_loss: 0.072 | test_loss: 0.099\n",
      "Epoch 184/300 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.108\n",
      "Epoch 184/300 | Batch 50/77 | train_loss: 0.073 | test_loss: 0.112\n",
      "Epoch 185/300 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.104\n",
      "Epoch 185/300 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.110\n",
      "Epoch 186/300 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.104\n",
      "Epoch 186/300 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.108\n",
      "Epoch 187/300 | Batch 0/77 | train_loss: 0.072 | test_loss: 0.106\n",
      "Epoch 187/300 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/300 | Batch 0/77 | train_loss: 0.071 | test_loss: 0.102\n",
      "Epoch 188/300 | Batch 50/77 | train_loss: 0.112 | test_loss: 0.097\n",
      "Epoch 189/300 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.107\n",
      "Epoch 189/300 | Batch 50/77 | train_loss: 0.082 | test_loss: 0.107\n",
      "Epoch 190/300 | Batch 0/77 | train_loss: 0.078 | test_loss: 0.114\n",
      "Epoch 190/300 | Batch 50/77 | train_loss: 0.080 | test_loss: 0.105\n",
      "Epoch 191/300 | Batch 0/77 | train_loss: 0.073 | test_loss: 0.103\n",
      "Epoch 191/300 | Batch 50/77 | train_loss: 0.076 | test_loss: 0.118\n",
      "Epoch 192/300 | Batch 0/77 | train_loss: 0.074 | test_loss: 0.104\n",
      "Epoch 192/300 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.128\n",
      "Epoch 193/300 | Batch 0/77 | train_loss: 0.070 | test_loss: 0.114\n",
      "Epoch 193/300 | Batch 50/77 | train_loss: 0.070 | test_loss: 0.114\n",
      "Epoch 194/300 | Batch 0/77 | train_loss: 0.069 | test_loss: 0.114\n",
      "Epoch 194/300 | Batch 50/77 | train_loss: 0.069 | test_loss: 0.115\n",
      "Epoch 195/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.110\n",
      "Epoch 195/300 | Batch 50/77 | train_loss: 0.068 | test_loss: 0.111\n",
      "Epoch 196/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.109\n",
      "Epoch 196/300 | Batch 50/77 | train_loss: 0.069 | test_loss: 0.113\n",
      "Epoch 197/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.105\n",
      "Epoch 197/300 | Batch 50/77 | train_loss: 0.068 | test_loss: 0.110\n",
      "Epoch 198/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.106\n",
      "Epoch 198/300 | Batch 50/77 | train_loss: 0.069 | test_loss: 0.113\n",
      "Epoch 199/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.102\n",
      "Epoch 199/300 | Batch 50/77 | train_loss: 0.068 | test_loss: 0.113\n",
      "Epoch 200/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.105\n",
      "Epoch 200/300 | Batch 50/77 | train_loss: 0.066 | test_loss: 0.108\n",
      "Epoch 201/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.105\n",
      "Epoch 201/300 | Batch 50/77 | train_loss: 0.067 | test_loss: 0.114\n",
      "Epoch 202/300 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.105\n",
      "Epoch 202/300 | Batch 50/77 | train_loss: 0.067 | test_loss: 0.110\n",
      "Epoch 203/300 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.101\n",
      "Epoch 203/300 | Batch 50/77 | train_loss: 0.066 | test_loss: 0.126\n",
      "Epoch 204/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.107\n",
      "Epoch 204/300 | Batch 50/77 | train_loss: 0.065 | test_loss: 0.108\n",
      "Epoch 205/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.113\n",
      "Epoch 205/300 | Batch 50/77 | train_loss: 0.066 | test_loss: 0.104\n",
      "Epoch 206/300 | Batch 0/77 | train_loss: 0.072 | test_loss: 0.109\n",
      "Epoch 206/300 | Batch 50/77 | train_loss: 0.066 | test_loss: 0.103\n",
      "Epoch 207/300 | Batch 0/77 | train_loss: 0.070 | test_loss: 0.109\n",
      "Epoch 207/300 | Batch 50/77 | train_loss: 0.080 | test_loss: 0.090\n",
      "Epoch 208/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.135\n",
      "Epoch 208/300 | Batch 50/77 | train_loss: 0.071 | test_loss: 0.137\n",
      "Epoch 209/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.101\n",
      "Epoch 209/300 | Batch 50/77 | train_loss: 0.087 | test_loss: 0.121\n",
      "Epoch 210/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.104\n",
      "Epoch 210/300 | Batch 50/77 | train_loss: 0.066 | test_loss: 0.097\n",
      "Epoch 211/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.106\n",
      "Epoch 211/300 | Batch 50/77 | train_loss: 0.065 | test_loss: 0.108\n",
      "Epoch 212/300 | Batch 0/77 | train_loss: 0.076 | test_loss: 0.109\n",
      "Epoch 212/300 | Batch 50/77 | train_loss: 0.066 | test_loss: 0.127\n",
      "Epoch 213/300 | Batch 0/77 | train_loss: 0.070 | test_loss: 0.113\n",
      "Epoch 213/300 | Batch 50/77 | train_loss: 0.063 | test_loss: 0.122\n",
      "Epoch 214/300 | Batch 0/77 | train_loss: 0.069 | test_loss: 0.109\n",
      "Epoch 214/300 | Batch 50/77 | train_loss: 0.078 | test_loss: 0.096\n",
      "Epoch 215/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.109\n",
      "Epoch 215/300 | Batch 50/77 | train_loss: 0.072 | test_loss: 0.130\n",
      "Epoch 216/300 | Batch 0/77 | train_loss: 0.064 | test_loss: 0.104\n",
      "Epoch 216/300 | Batch 50/77 | train_loss: 0.065 | test_loss: 0.114\n",
      "Epoch 217/300 | Batch 0/77 | train_loss: 0.065 | test_loss: 0.107\n",
      "Epoch 217/300 | Batch 50/77 | train_loss: 0.067 | test_loss: 0.119\n",
      "Epoch 218/300 | Batch 0/77 | train_loss: 0.064 | test_loss: 0.103\n",
      "Epoch 218/300 | Batch 50/77 | train_loss: 0.067 | test_loss: 0.121\n",
      "Epoch 219/300 | Batch 0/77 | train_loss: 0.062 | test_loss: 0.105\n",
      "Epoch 219/300 | Batch 50/77 | train_loss: 0.065 | test_loss: 0.119\n",
      "Epoch 220/300 | Batch 0/77 | train_loss: 0.064 | test_loss: 0.112\n",
      "Epoch 220/300 | Batch 50/77 | train_loss: 0.062 | test_loss: 0.102\n",
      "Epoch 221/300 | Batch 0/77 | train_loss: 0.063 | test_loss: 0.102\n",
      "Epoch 221/300 | Batch 50/77 | train_loss: 0.062 | test_loss: 0.117\n",
      "Epoch 222/300 | Batch 0/77 | train_loss: 0.065 | test_loss: 0.115\n",
      "Epoch 222/300 | Batch 50/77 | train_loss: 0.060 | test_loss: 0.126\n",
      "Epoch 223/300 | Batch 0/77 | train_loss: 0.064 | test_loss: 0.110\n",
      "Epoch 223/300 | Batch 50/77 | train_loss: 0.059 | test_loss: 0.105\n",
      "Epoch 224/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.121\n",
      "Epoch 224/300 | Batch 50/77 | train_loss: 0.063 | test_loss: 0.132\n",
      "Epoch 225/300 | Batch 0/77 | train_loss: 0.063 | test_loss: 0.112\n",
      "Epoch 225/300 | Batch 50/77 | train_loss: 0.059 | test_loss: 0.110\n",
      "Epoch 226/300 | Batch 0/77 | train_loss: 0.067 | test_loss: 0.123\n",
      "Epoch 226/300 | Batch 50/77 | train_loss: 0.062 | test_loss: 0.101\n",
      "Epoch 227/300 | Batch 0/77 | train_loss: 0.064 | test_loss: 0.116\n",
      "Epoch 227/300 | Batch 50/77 | train_loss: 0.060 | test_loss: 0.102\n",
      "Epoch 228/300 | Batch 0/77 | train_loss: 0.060 | test_loss: 0.104\n",
      "Epoch 228/300 | Batch 50/77 | train_loss: 0.060 | test_loss: 0.145\n",
      "Epoch 229/300 | Batch 0/77 | train_loss: 0.063 | test_loss: 0.105\n",
      "Epoch 229/300 | Batch 50/77 | train_loss: 0.060 | test_loss: 0.106\n",
      "Epoch 230/300 | Batch 0/77 | train_loss: 0.060 | test_loss: 0.102\n",
      "Epoch 230/300 | Batch 50/77 | train_loss: 0.058 | test_loss: 0.133\n",
      "Epoch 231/300 | Batch 0/77 | train_loss: 0.062 | test_loss: 0.112\n",
      "Epoch 231/300 | Batch 50/77 | train_loss: 0.058 | test_loss: 0.108\n",
      "Epoch 232/300 | Batch 0/77 | train_loss: 0.062 | test_loss: 0.113\n",
      "Epoch 232/300 | Batch 50/77 | train_loss: 0.056 | test_loss: 0.111\n",
      "Epoch 233/300 | Batch 0/77 | train_loss: 0.059 | test_loss: 0.114\n",
      "Epoch 233/300 | Batch 50/77 | train_loss: 0.059 | test_loss: 0.108\n",
      "Epoch 234/300 | Batch 0/77 | train_loss: 0.059 | test_loss: 0.107\n",
      "Epoch 234/300 | Batch 50/77 | train_loss: 0.077 | test_loss: 0.102\n",
      "Epoch 235/300 | Batch 0/77 | train_loss: 0.058 | test_loss: 0.109\n",
      "Epoch 235/300 | Batch 50/77 | train_loss: 0.060 | test_loss: 0.101\n",
      "Epoch 236/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.114\n",
      "Epoch 236/300 | Batch 50/77 | train_loss: 0.060 | test_loss: 0.115\n",
      "Epoch 237/300 | Batch 0/77 | train_loss: 0.093 | test_loss: 0.126\n",
      "Epoch 237/300 | Batch 50/77 | train_loss: 0.091 | test_loss: 0.115\n",
      "Epoch 238/300 | Batch 0/77 | train_loss: 0.082 | test_loss: 0.122\n",
      "Epoch 238/300 | Batch 50/77 | train_loss: 0.060 | test_loss: 0.108\n",
      "Epoch 239/300 | Batch 0/77 | train_loss: 0.062 | test_loss: 0.111\n",
      "Epoch 239/300 | Batch 50/77 | train_loss: 0.062 | test_loss: 0.107\n",
      "Epoch 240/300 | Batch 0/77 | train_loss: 0.061 | test_loss: 0.103\n",
      "Epoch 240/300 | Batch 50/77 | train_loss: 0.064 | test_loss: 0.102\n",
      "Epoch 241/300 | Batch 0/77 | train_loss: 0.065 | test_loss: 0.124\n",
      "Epoch 241/300 | Batch 50/77 | train_loss: 0.056 | test_loss: 0.102\n",
      "Epoch 242/300 | Batch 0/77 | train_loss: 0.063 | test_loss: 0.143\n",
      "Epoch 242/300 | Batch 50/77 | train_loss: 0.055 | test_loss: 0.104\n",
      "Epoch 243/300 | Batch 0/77 | train_loss: 0.062 | test_loss: 0.123\n",
      "Epoch 243/300 | Batch 50/77 | train_loss: 0.055 | test_loss: 0.104\n",
      "Epoch 244/300 | Batch 0/77 | train_loss: 0.058 | test_loss: 0.120\n",
      "Epoch 244/300 | Batch 50/77 | train_loss: 0.055 | test_loss: 0.106\n",
      "Epoch 245/300 | Batch 0/77 | train_loss: 0.057 | test_loss: 0.106\n",
      "Epoch 245/300 | Batch 50/77 | train_loss: 0.053 | test_loss: 0.102\n",
      "Epoch 246/300 | Batch 0/77 | train_loss: 0.057 | test_loss: 0.107\n",
      "Epoch 246/300 | Batch 50/77 | train_loss: 0.054 | test_loss: 0.107\n",
      "Epoch 247/300 | Batch 0/77 | train_loss: 0.056 | test_loss: 0.106\n",
      "Epoch 247/300 | Batch 50/77 | train_loss: 0.054 | test_loss: 0.103\n",
      "Epoch 248/300 | Batch 0/77 | train_loss: 0.056 | test_loss: 0.105\n",
      "Epoch 248/300 | Batch 50/77 | train_loss: 0.058 | test_loss: 0.100\n",
      "Epoch 249/300 | Batch 0/77 | train_loss: 0.055 | test_loss: 0.104\n",
      "Epoch 249/300 | Batch 50/77 | train_loss: 0.058 | test_loss: 0.101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/300 | Batch 0/77 | train_loss: 0.056 | test_loss: 0.110\n",
      "Epoch 250/300 | Batch 50/77 | train_loss: 0.055 | test_loss: 0.102\n",
      "Epoch 251/300 | Batch 0/77 | train_loss: 0.058 | test_loss: 0.118\n",
      "Epoch 251/300 | Batch 50/77 | train_loss: 0.057 | test_loss: 0.102\n",
      "Epoch 252/300 | Batch 0/77 | train_loss: 0.060 | test_loss: 0.139\n",
      "Epoch 252/300 | Batch 50/77 | train_loss: 0.058 | test_loss: 0.104\n",
      "Epoch 253/300 | Batch 0/77 | train_loss: 0.059 | test_loss: 0.152\n",
      "Epoch 253/300 | Batch 50/77 | train_loss: 0.068 | test_loss: 0.110\n",
      "Epoch 254/300 | Batch 0/77 | train_loss: 0.086 | test_loss: 0.133\n",
      "Epoch 254/300 | Batch 50/77 | train_loss: 0.055 | test_loss: 0.124\n",
      "Epoch 255/300 | Batch 0/77 | train_loss: 0.072 | test_loss: 0.187\n",
      "Epoch 255/300 | Batch 50/77 | train_loss: 0.060 | test_loss: 0.102\n",
      "Epoch 256/300 | Batch 0/77 | train_loss: 0.065 | test_loss: 0.117\n",
      "Epoch 256/300 | Batch 50/77 | train_loss: 0.057 | test_loss: 0.103\n",
      "Epoch 257/300 | Batch 0/77 | train_loss: 0.058 | test_loss: 0.116\n",
      "Epoch 257/300 | Batch 50/77 | train_loss: 0.056 | test_loss: 0.101\n",
      "Epoch 258/300 | Batch 0/77 | train_loss: 0.058 | test_loss: 0.108\n",
      "Epoch 258/300 | Batch 50/77 | train_loss: 0.057 | test_loss: 0.107\n",
      "Epoch 259/300 | Batch 0/77 | train_loss: 0.060 | test_loss: 0.106\n",
      "Epoch 259/300 | Batch 50/77 | train_loss: 0.059 | test_loss: 0.111\n",
      "Epoch 260/300 | Batch 0/77 | train_loss: 0.063 | test_loss: 0.104\n",
      "Epoch 260/300 | Batch 50/77 | train_loss: 0.059 | test_loss: 0.113\n",
      "Epoch 261/300 | Batch 0/77 | train_loss: 0.063 | test_loss: 0.105\n",
      "Epoch 261/300 | Batch 50/77 | train_loss: 0.056 | test_loss: 0.111\n",
      "Epoch 262/300 | Batch 0/77 | train_loss: 0.060 | test_loss: 0.106\n",
      "Epoch 262/300 | Batch 50/77 | train_loss: 0.066 | test_loss: 0.115\n",
      "Epoch 263/300 | Batch 0/77 | train_loss: 0.060 | test_loss: 0.103\n",
      "Epoch 263/300 | Batch 50/77 | train_loss: 0.056 | test_loss: 0.120\n",
      "Epoch 264/300 | Batch 0/77 | train_loss: 0.060 | test_loss: 0.101\n",
      "Epoch 264/300 | Batch 50/77 | train_loss: 0.051 | test_loss: 0.101\n",
      "Epoch 265/300 | Batch 0/77 | train_loss: 0.066 | test_loss: 0.113\n",
      "Epoch 265/300 | Batch 50/77 | train_loss: 0.051 | test_loss: 0.100\n",
      "Epoch 266/300 | Batch 0/77 | train_loss: 0.055 | test_loss: 0.107\n",
      "Epoch 266/300 | Batch 50/77 | train_loss: 0.050 | test_loss: 0.103\n",
      "Epoch 267/300 | Batch 0/77 | train_loss: 0.054 | test_loss: 0.101\n",
      "Epoch 267/300 | Batch 50/77 | train_loss: 0.051 | test_loss: 0.100\n",
      "Epoch 268/300 | Batch 0/77 | train_loss: 0.054 | test_loss: 0.102\n",
      "Epoch 268/300 | Batch 50/77 | train_loss: 0.049 | test_loss: 0.101\n",
      "Epoch 269/300 | Batch 0/77 | train_loss: 0.053 | test_loss: 0.100\n",
      "Epoch 269/300 | Batch 50/77 | train_loss: 0.049 | test_loss: 0.100\n",
      "Epoch 270/300 | Batch 0/77 | train_loss: 0.053 | test_loss: 0.100\n",
      "Epoch 270/300 | Batch 50/77 | train_loss: 0.049 | test_loss: 0.100\n",
      "Epoch 271/300 | Batch 0/77 | train_loss: 0.052 | test_loss: 0.100\n",
      "Epoch 271/300 | Batch 50/77 | train_loss: 0.048 | test_loss: 0.101\n",
      "Epoch 272/300 | Batch 0/77 | train_loss: 0.052 | test_loss: 0.102\n",
      "Epoch 272/300 | Batch 50/77 | train_loss: 0.049 | test_loss: 0.101\n",
      "Epoch 273/300 | Batch 0/77 | train_loss: 0.051 | test_loss: 0.100\n",
      "Epoch 273/300 | Batch 50/77 | train_loss: 0.048 | test_loss: 0.102\n",
      "Epoch 274/300 | Batch 0/77 | train_loss: 0.051 | test_loss: 0.101\n",
      "Epoch 274/300 | Batch 50/77 | train_loss: 0.048 | test_loss: 0.102\n",
      "Epoch 275/300 | Batch 0/77 | train_loss: 0.051 | test_loss: 0.103\n",
      "Epoch 275/300 | Batch 50/77 | train_loss: 0.049 | test_loss: 0.106\n",
      "Epoch 276/300 | Batch 0/77 | train_loss: 0.051 | test_loss: 0.103\n",
      "Epoch 276/300 | Batch 50/77 | train_loss: 0.048 | test_loss: 0.102\n",
      "Epoch 277/300 | Batch 0/77 | train_loss: 0.050 | test_loss: 0.102\n",
      "Epoch 277/300 | Batch 50/77 | train_loss: 0.049 | test_loss: 0.104\n",
      "Epoch 278/300 | Batch 0/77 | train_loss: 0.050 | test_loss: 0.103\n",
      "Epoch 278/300 | Batch 50/77 | train_loss: 0.047 | test_loss: 0.105\n",
      "Epoch 279/300 | Batch 0/77 | train_loss: 0.049 | test_loss: 0.104\n",
      "Epoch 279/300 | Batch 50/77 | train_loss: 0.046 | test_loss: 0.103\n",
      "Epoch 280/300 | Batch 0/77 | train_loss: 0.048 | test_loss: 0.103\n",
      "Epoch 280/300 | Batch 50/77 | train_loss: 0.046 | test_loss: 0.106\n",
      "Epoch 281/300 | Batch 0/77 | train_loss: 0.049 | test_loss: 0.104\n",
      "Epoch 281/300 | Batch 50/77 | train_loss: 0.047 | test_loss: 0.105\n",
      "Epoch 282/300 | Batch 0/77 | train_loss: 0.048 | test_loss: 0.106\n",
      "Epoch 282/300 | Batch 50/77 | train_loss: 0.047 | test_loss: 0.108\n",
      "Epoch 283/300 | Batch 0/77 | train_loss: 0.048 | test_loss: 0.106\n",
      "Epoch 283/300 | Batch 50/77 | train_loss: 0.048 | test_loss: 0.106\n",
      "Epoch 284/300 | Batch 0/77 | train_loss: 0.068 | test_loss: 0.113\n",
      "Epoch 284/300 | Batch 50/77 | train_loss: 0.049 | test_loss: 0.112\n",
      "Epoch 285/300 | Batch 0/77 | train_loss: 0.054 | test_loss: 0.147\n",
      "Epoch 285/300 | Batch 50/77 | train_loss: 0.050 | test_loss: 0.108\n",
      "Epoch 286/300 | Batch 0/77 | train_loss: 0.050 | test_loss: 0.096\n",
      "Epoch 286/300 | Batch 50/77 | train_loss: 0.049 | test_loss: 0.102\n",
      "Epoch 287/300 | Batch 0/77 | train_loss: 0.049 | test_loss: 0.102\n",
      "Epoch 287/300 | Batch 50/77 | train_loss: 0.052 | test_loss: 0.103\n",
      "Epoch 288/300 | Batch 0/77 | train_loss: 0.048 | test_loss: 0.106\n",
      "Epoch 288/300 | Batch 50/77 | train_loss: 0.046 | test_loss: 0.103\n",
      "Epoch 289/300 | Batch 0/77 | train_loss: 0.047 | test_loss: 0.105\n",
      "Epoch 289/300 | Batch 50/77 | train_loss: 0.045 | test_loss: 0.104\n",
      "Epoch 290/300 | Batch 0/77 | train_loss: 0.046 | test_loss: 0.106\n",
      "Epoch 290/300 | Batch 50/77 | train_loss: 0.045 | test_loss: 0.105\n",
      "Epoch 291/300 | Batch 0/77 | train_loss: 0.047 | test_loss: 0.106\n",
      "Epoch 291/300 | Batch 50/77 | train_loss: 0.044 | test_loss: 0.106\n",
      "Epoch 292/300 | Batch 0/77 | train_loss: 0.047 | test_loss: 0.104\n",
      "Epoch 292/300 | Batch 50/77 | train_loss: 0.046 | test_loss: 0.112\n",
      "Epoch 293/300 | Batch 0/77 | train_loss: 0.047 | test_loss: 0.104\n",
      "Epoch 293/300 | Batch 50/77 | train_loss: 0.044 | test_loss: 0.104\n",
      "Epoch 294/300 | Batch 0/77 | train_loss: 0.046 | test_loss: 0.103\n",
      "Epoch 294/300 | Batch 50/77 | train_loss: 0.044 | test_loss: 0.105\n",
      "Epoch 295/300 | Batch 0/77 | train_loss: 0.045 | test_loss: 0.105\n",
      "Epoch 295/300 | Batch 50/77 | train_loss: 0.043 | test_loss: 0.106\n",
      "Epoch 296/300 | Batch 0/77 | train_loss: 0.045 | test_loss: 0.104\n",
      "Epoch 296/300 | Batch 50/77 | train_loss: 0.044 | test_loss: 0.106\n",
      "Epoch 297/300 | Batch 0/77 | train_loss: 0.045 | test_loss: 0.104\n",
      "Epoch 297/300 | Batch 50/77 | train_loss: 0.043 | test_loss: 0.107\n",
      "Epoch 298/300 | Batch 0/77 | train_loss: 0.044 | test_loss: 0.105\n",
      "Epoch 298/300 | Batch 50/77 | train_loss: 0.047 | test_loss: 0.116\n",
      "Epoch 299/300 | Batch 0/77 | train_loss: 0.046 | test_loss: 0.105\n",
      "Epoch 299/300 | Batch 50/77 | train_loss: 0.043 | test_loss: 0.107\n",
      "Epoch 300/300 | Batch 0/77 | train_loss: 0.044 | test_loss: 0.107\n",
      "Epoch 300/300 | Batch 50/77 | train_loss: 0.061 | test_loss: 0.102\n",
      "\n",
      "Source\n",
      "IN: c o m m o n <EOS>\n",
      "\n",
      "Target\n",
      "OUT: c m m m o o <EOS>\n",
      "\n",
      "Source\n",
      "IN: a p p l e <EOS>\n",
      "\n",
      "Target\n",
      "OUT: a e l p p <EOS>\n",
      "\n",
      "Source\n",
      "IN: z h e d o n g <EOS>\n",
      "\n",
      "Target\n",
      "OUT: d e g h n o z <EOS>\n"
     ]
    }
   ],
   "source": [
    "from pointer_net import PointerNetwork\n",
    "import sys\n",
    "import numpy as np\n",
    "if int(sys.version[0]) == 2:\n",
    "    from io import open\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "# end function\n",
    "\n",
    "\n",
    "def build_map(data):\n",
    "    specials = ['<GO>',  '<EOS>', '<PAD>', '<UNK>']\n",
    "    chars = list(set([char for line in data.split('\\n') for char in line]))\n",
    "    chars = sorted(chars)\n",
    "    idx2char = {idx: char for idx, char in enumerate(specials+chars)}\n",
    "    char2idx = {char: idx for idx, char in idx2char.items()}\n",
    "    return idx2char, char2idx\n",
    "# end function\n",
    "\n",
    "\n",
    "def preprocess_data(max_len):\n",
    "    X_data = read_data('temp/letters_source.txt')\n",
    "    Y_data = read_data('temp/letters_target.txt')\n",
    "\n",
    "    X_idx2char, X_char2idx = build_map(X_data)\n",
    "    print(\"==> Word Index Built\")\n",
    "\n",
    "    x_unk = X_char2idx['<UNK>']\n",
    "    x_eos = X_char2idx['<EOS>']\n",
    "    x_pad = X_char2idx['<PAD>']\n",
    "\n",
    "    X_indices = []\n",
    "    X_seq_len = []\n",
    "    Y_indices = []\n",
    "    Y_seq_len = []\n",
    "\n",
    "    for x_line, y_line in zip(X_data.split('\\n'), Y_data.split('\\n')):\n",
    "        x_chars = [X_char2idx.get(char, x_unk) for char in x_line]\n",
    "        _x_chars = x_chars + [x_eos] + [x_pad]* (max_len-1-len(x_chars))\n",
    "        \n",
    "        y_chars = [X_char2idx.get(char, x_unk) for char in y_line]\n",
    "        _y_chars = y_chars + [x_eos] + [x_pad]* (max_len-1-len(y_chars))\n",
    "        target = [_x_chars.index(y) for y in _y_chars] # we are predicting the positions\n",
    "\n",
    "        X_indices.append(_x_chars)\n",
    "        Y_indices.append(target)\n",
    "        X_seq_len.append(len(x_chars)+1)\n",
    "        Y_seq_len.append(len(y_chars)+1)\n",
    "\n",
    "    X_indices = np.array(X_indices)\n",
    "    Y_indices = np.array(Y_indices)\n",
    "    X_seq_len = np.array(X_seq_len)\n",
    "    Y_seq_len = np.array(Y_seq_len)\n",
    "    print(\"==> Sequence Padded\")\n",
    "\n",
    "    return X_indices, X_seq_len, Y_indices, Y_seq_len, X_char2idx, X_idx2char\n",
    "# end function\n",
    "\n",
    "\n",
    "def train_test_split(X_indices, X_seq_len, Y_indices, Y_seq_len, BATCH_SIZE):\n",
    "    X_train = X_indices[BATCH_SIZE:]\n",
    "    X_train_len = X_seq_len[BATCH_SIZE:]\n",
    "    Y_train = Y_indices[BATCH_SIZE:]\n",
    "    Y_train_len = Y_seq_len[BATCH_SIZE:]\n",
    "\n",
    "    X_test = X_indices[:BATCH_SIZE]\n",
    "    X_test_len = X_seq_len[:BATCH_SIZE]\n",
    "    Y_test = Y_indices[:BATCH_SIZE]\n",
    "    Y_test_len = Y_seq_len[:BATCH_SIZE]\n",
    "\n",
    "    return (X_train, X_train_len, Y_train, Y_train_len), (X_test, X_test_len, Y_test, Y_test_len)\n",
    "# end function\n",
    "\n",
    "\n",
    "def main():\n",
    "    BATCH_SIZE = 128\n",
    "    MAX_LEN = 15\n",
    "    X_indices, X_seq_len, Y_indices, Y_seq_len, X_char2idx, X_idx2char = preprocess_data(MAX_LEN)\n",
    "    \n",
    "    (X_train, X_train_len, Y_train, Y_train_len), (X_test, X_test_len, Y_test, Y_test_len) \\\n",
    "        = train_test_split(X_indices, X_seq_len, Y_indices, Y_seq_len, BATCH_SIZE)\n",
    "    \n",
    "    model = PointerNetwork(\n",
    "        max_len = MAX_LEN,\n",
    "        rnn_size = 50,\n",
    "        X_word2idx = X_char2idx,\n",
    "        embedding_dim = 15)\n",
    "    \n",
    "    model.fit(X_train, X_train_len, Y_train, Y_train_len,\n",
    "        val_data=(X_test, X_test_len, Y_test, Y_test_len), batch_size=BATCH_SIZE, n_epoch=300)\n",
    "    model.infer('common', X_idx2char)\n",
    "    model.infer('apple', X_idx2char)\n",
    "    model.infer('zhedong', X_idx2char)\n",
    "# end main\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
